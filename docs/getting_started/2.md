# Seed the database tables

## Create the tables

Create the tables in a database called `bank`.

### PostgreSQL

```sql
postgres=# CREATE DATABASE bank;
```

Now, you need to disconnect and reconnect to `bank`

```bash
psql 'postgres://fabio:postgres@localhost:5432/bank?sslmode=disable'
```

Once on the SQL prompt, import the file, or copy-paste, as you prefer.

```sql
bank=# \i bank.sql

bank=# \d
--          List of relations
--  Schema |   Name   | Type  | Owner 
-- --------+----------+-------+-------
--  public | orders   | table | fabio
--  public | ref_data | table | fabio
-- (2 rows)

```

### CockroachDB

```sql
defaultdb> CREATE DATABASE bank;

defaultdb> USE bank;

bank> \i bank.sql

bank> SHOW TABLES;
--   schema_name | table_name | type  |   owner   | estimated_row_count | locality
-- --------------+------------+-------+-----------+---------------------+-----------
--   public      | orders     | table | cockroach |                   0 | NULL
--   public      | ref_data   | table | cockroach |                   0 | NULL
-- (2 rows)

```

## Generate datasets

Next, generate some CSV data to seed the database.
`dbworkload` has 2 built-in utility functions that can assist with this task:

- `dbworkload util yaml`, which converts a DDL file into a _data generation definition_ file, structured in YAML.
- `dbworkload util csv`, which takes the YAML file as input and outputs CSV files.

Read the docs for util commands [`yaml`](../util/yaml.md) and [`csv`](../util/csv.md) for more information.

Let's use the `yaml` utility with our `bank.ddl` file.

```bash
dbworkload util yaml -i bank.ddl
```

For this exercise, we will use below simplified YAML file.
Replace the content of `bank.yaml` with below YAML

```yaml
ref_data:
- count: 1000
  sort-by: 
    - acc_no
  columns:
    acc_no:
      type: sequence
      args:
        start: 0
    external_ref_id:
      type: UUIDv4
      args: {}
    created_time:
      type: timestamp
      args:
        start: '2000-01-01'
        end: '2024-12-31'
        format: '%Y-%m-%d %H:%M:%S.%f'
    acc_details:
      type: string
      args: 
        min: 10
        max: 30
```

Now let's create a CSV dataset

```bash
dbworkload util csv -i bank.yaml -x 1
```

The CSV files will be located inside a `bank` directory.

```bash
$ ls -lh1 bank
ref_data.0_0_0.tsv

$ wc -l bank/*
    1000 bank/ref_data.0_0_0.tsv
```

Inspect it

```bash
$ head -n5 bank/ref_data.0_0_0.tsv 
0       3a2edc9d-a96b-4541-99ae-0098527545f7    2008-03-19 06:20:27.209214      CWUh0FWashpmWCx4LF3kb1
1       829de6d6-103c-4707-9668-c4359ef5373c    2014-02-13 22:04:20.168239      QGspICZBHYpRLnHNcg
2       5dd183af-d728-4e12-8b11-2900b6f6880a    2019-04-01 16:14:40.388236      sEUukccOePdnIbiQyVUSi0HS7rL
3       21f00778-5fca-4302-8380-56fa461adfc8    2003-05-21 19:21:21.598455      OQTNwxoZIAdNmcA6fJM5eGDvMJgKJ
4       035dac61-b4a3-40a4-9e4d-0deb50fef3ae    2011-08-15 06:15:40.405698      RvToVnn20BEXoxFzw9QFpCt
```

## Importing datasets

Now we are ready to import the CSV file into our table `ref_data`.

### PostgreSQL

For PostgreSQL Server, at the SQL prompt, just use `COPY`

```sql
bank=# COPY ref_data FROM '/path/to/workloads/bank/ref_data.0_0_0.csv' WITH CSV DELIMITER AS e'\t';
COPY 1000
Time: 8.713 ms
```

### CockroachDB

For CockroachDB, my favorite method is to use a webserver for serving the CSV files.

Open a new terminal then start a simple python server

```bash
cd workloads/bank
python3 -m http.server 3000
```

If you open your browser at <http://localhost:3000>, you should see file `ref_data.0_0_0.tsv` being served.

At the SQL prompt, import the file

```sql
bank> IMPORT INTO ref_data CSV DATA ('http://localhost:3000/ref_data.0_0_0.tsv') WITH delimiter = e'\t', nullif = '';
        job_id        |  status   | fraction_completed | rows | index_entries | bytes
----------------------+-----------+--------------------+------+---------------+--------
  1013454367369822209 | succeeded |                  1 | 1000 |             0 | 71401
(1 row)
```
